{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m spaces\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, p1, p2):\n",
    "        self.observation_space = spaces.Discrete(BOARD_COLS*BOARD_ROWS)  # can also be configuration of the board - large\n",
    "        self.action_space = spaces.Discrete(BOARD_COLS*BOARD_ROWS)\n",
    "        self.board = np.zeros((BOARD_COLS, BOARD_ROWS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "        self.player_symbols = {1: \"Player 1\", -1: \"Player 2\", 0:\"Tie\"}\n",
    "        self.current_player = self.p1.symbol = 1  # first player\n",
    "        self.p2.symbol = -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_COLS, BOARD_ROWS))\n",
    "        self.done = False\n",
    "        self.current_player = 1\n",
    "        return self.board\n",
    "    \n",
    "    def available_positions(self):\n",
    "        return list(zip(*np.where(self.board == 0)))\n",
    "\n",
    "    def update_state(self, position):\n",
    "        self.board[position] = self.current_player\n",
    "        self.current_player  = -self.current_player\n",
    "\n",
    "    def check_game_status(self):\n",
    "        vertical = np.sum(self.board, 0)\n",
    "        horizontal = np.sum(self.board, 1)\n",
    "        diag = np.sum(np.diag(self.board))\n",
    "        antidiag = np.sum(np.diag(np.fliplr(self.board)))\n",
    "\n",
    "        if any(vertical == 3) or any(horizontal==3) or diag==3 or antidiag==3 : \n",
    "            self.done = True\n",
    "            return 1\n",
    "        \n",
    "        if any(vertical == -3) or any(horizontal==-3)or diag==-3 or antidiag==-3: \n",
    "            self.done = True\n",
    "            return -1\n",
    "\n",
    "        if np.all(self.board):  # if board is filled\n",
    "            self.done = True\n",
    "            return 0  # tie\n",
    "        \n",
    "        self.done = False\n",
    "        return None\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.board, 0 , True, None\n",
    "        \n",
    "        self.update_state(action)\n",
    "        status = self.check_game_status()\n",
    "        if status is not None:\n",
    "            reward = status\n",
    "            info = {\"result\": self.player_symbols[status]}\n",
    "            return self.board, reward, self.done, info\n",
    "        \n",
    "        return self.board, None, self.done, None\n",
    "        \n",
    "    def render(self, mode=\"human\"):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                token = ''\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "\n",
    "        print('-------------')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.symbol = 1 # modified when assigned to the game\n",
    "    \n",
    "    def act(self, positions, board=None):\n",
    "        idx = np.random.choice(len(positions))\n",
    "        return positions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    def act(self, positions, current_board=None):\n",
    "        while True:\n",
    "            matrix = {1: (0, 0), 2: (0, 1), 3: (0, 2),\n",
    "                      4: (1, 0), 5: (1, 1), 6: (1, 2),\n",
    "                      7: (2, 0), 8: (2, 1), 9: (2, 2)}\n",
    "            user_input = int(input(\"Input your action 1-9:\"))\n",
    "            pos = matrix[user_input]\n",
    "            if pos in positions:\n",
    "                return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(P1, P2, render=False):\n",
    "    agents = [P1, P2]\n",
    "    env = TicTacToe(*agents)\n",
    "    env.reset()\n",
    "    if render: env.render()\n",
    "    \n",
    "    while not env.done:\n",
    "        for agent in agents:\n",
    "            action = agent.act(env.available_positions(), env.board)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if render: \n",
    "                print(\"place \", action)\n",
    "                env.render()\n",
    "            if done:\n",
    "                if render: \n",
    "                    print(info['result'])\n",
    "                return reward\n",
    "\n",
    "\n",
    "play(Player(\"P1\"), HumanPlayer(\"P2\"), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def win_stats(P1,P2, n=10000):\n",
    "    print(f\"Playing {n} games\")\n",
    "    game_stats = [play(P1,P2) for i in range(n)]\n",
    "    \n",
    "    print(\"P1 Wins: \",game_stats.count(1))\n",
    "    print(\"P2 Wins: \",game_stats.count(-1))\n",
    "    print(\"Ties: \",game_stats.count(0))\n",
    "\n",
    "\n",
    "win_stats(Player(\"P1\"), Player(\"P2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class QAgent(Player):\n",
    "    def __init__(self, name, policy=None):\n",
    "        self.name = name\n",
    "        self.symbol = 1 \n",
    "        self.α = .2\n",
    "        self.γ = .9\n",
    "        self.ϵ = .3 \n",
    "        self.states = []  # save all taken positions\n",
    "        self.Q_value = {} # {state-action: values}}, generated/initialized on the fly\n",
    "        if policy is not None:\n",
    "            with open(policy, 'rb') as fr:\n",
    "                self.Q_value = pickle.load(fr)\n",
    "\n",
    "    def add_state(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def board2vec(self, board):\n",
    "        return str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        \n",
    "\n",
    "    def max_q(self, board, positions):\n",
    "        action = None\n",
    "        # Initialize Q\n",
    "        value_max = -999 \n",
    "        # choose Actions from Q \n",
    "        for p in positions:\n",
    "            next_board = board.copy()\n",
    "            next_board[p] = self.symbol\n",
    "            next_board_vector = self.board2vec(next_board)\n",
    "            action_value = 0 if self.Q_value.get(next_board_vector) is None else self.Q_value.get(next_board_vector)\n",
    "            if action_value >= value_max:\n",
    "                value_max = action_value\n",
    "                action = p\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def act(self, positions, current_board):\n",
    "        if np.random.uniform(0,1) <= self.ϵ:               # Explore\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            action = self.max_q(current_board, positions)  # Exploit\n",
    "        return action\n",
    "\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feed_reward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.Q_value.get(st) is None:\n",
    "                self.Q_value[st] = 0\n",
    "            self.Q_value[st] += self.α * (self.γ * reward - self.Q_value[st])\n",
    "            reward = self.Q_value[st]\n",
    "\n",
    "\n",
    "    def save_policy(self):\n",
    "        with open('policy_' + str(self.name), 'wb') as fw:\n",
    "            pickle.dump(self.Q_value, fw)\n",
    "\n",
    "    \n",
    "    def load_policy(self, file):\n",
    "        with open(file, 'rb') as fr:\n",
    "            self.Q_value = pickle.load(fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=10000):\n",
    "    p1 = QAgent(\"p1\")\n",
    "    p2 = QAgent(\"p2\")\n",
    "    agents = [p1, p2]\n",
    "    env = TicTacToe(p1, p2)\n",
    "    for i in range(n_episodes):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Episode {}\".format(i))\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for agent in agents:\n",
    "                if not done:\n",
    "                    action = agent.act(env.available_positions(), env.board)\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                    agent.add_state(agent.board2vec(state))\n",
    "                    if done:\n",
    "                        if reward == 1:\n",
    "                            p1.feed_reward(1)\n",
    "                            p2.feed_reward(0)\n",
    "                        elif reward == -1:\n",
    "                            p1.feed_reward(0)\n",
    "                            p2.feed_reward(1)\n",
    "                        else:\n",
    "                            p1.feed_reward(.1)\n",
    "                            p2.feed_reward(.5)\n",
    "                        p1.reset()\n",
    "                        p2.reset()\n",
    "        env.reset()\n",
    "    p1.save_policy()\n",
    "    p2.save_policy()\n",
    "\n",
    "train(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(QAgent(\"p1\",policy='policy_p1'),Player(\"P2\"),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = QAgent(\"p1\",policy='policy_p1')\n",
    "P2 = Player(\"P2\")\n",
    "\n",
    "win_stats(P1,P2, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = Player(\"P1\")\n",
    "P2 = QAgent(\"p2\",policy='policy_p2')\n",
    "\n",
    "win_stats(P1, P2, n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(HumanPlayer(\"P1\"), Player(\"P2\"),True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8623a6d2b2f684a36d917583bbf9fb4dd3cd9028024f04425ddc24ab2eb2e9a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
